{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelly790/.conda/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for graphing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID는 필요없어서 제거\n",
    "df = pd.read_csv('/home/kelly790/dacon 연습/문장분류/train.csv')\n",
    "df.drop(columns=['ID'], inplace=True)\n",
    "\n",
    "test = pd.read_csv('/home/kelly790/dacon 연습/문장분류/test.csv')\n",
    "test.drop(columns=['ID'], inplace=True)\n",
    "\n",
    "submission = pd.read_csv('/home/kelly790/dacon 연습/문장분류/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline과 같은 기본 seed 고정 및 device cuda 설정\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "CFG = {\n",
    "    'EPOCHS':20,\n",
    "    'LEARNING_RATE':1e-5,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':41\n",
    "}\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정\n",
    "device = torch.device('cuda:1')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 split\n",
    "train,val,_,_ = train_test_split(df, df['label'], test_size=0.2, random_state=CFG['SEED'])\n",
    "\n",
    "#train, valid 인덱스 섞여있으므로 reset\n",
    "train = train.reset_index(drop=True)\n",
    "valid = val.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface 모델 검색 \\\n",
    "-> https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#klue/roberta-small 사용\n",
    "\n",
    "model = 'klue/roberta-small'\n",
    "base_model = AutoModel.from_pretrained(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTypeDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, labels=None): #x,y가 뭔지 정의할 것임 ; dataframe, tokenizer, label은 parameter로\n",
    "        texts = dataframe['문장'].values.tolist() #문장을 모두 리스트화\n",
    "\n",
    "        self.texts = [tokenizer(text, padding='max_length', max_length=90, truncation=True, return_tensors='pt') for text in texts] \n",
    "        #텍스트마다 토큰화, 문장 길이가 다를 수 있어서 90으로 제한, 그보다 작은 건 padding으로 채우기, truncation으로 긴 문장 자르기, return_tensors는 tensor로 tokenizer 값 리턴\n",
    "        \n",
    "        self.labels = labels #라벨은 그대로\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts) #text의 길이 리턴\n",
    "\n",
    "    def __getitem__(self, idx): #idx에 해당되는 x,y를 리턴\n",
    "        text = self.texts[idx] #x는 self.text에서 가져옴\n",
    "\n",
    "        if self.labels is not None: #y는 텍스트(x)에서 해당되는 type, polarity, tense, certainty 가져옴\n",
    "            type_tmp = self.labels['type'][idx]\n",
    "            polarity_tmp = self.labels['polarity'][idx]\n",
    "            tense_tmp = self.labels['tense'][idx]\n",
    "            certainty_tmp = self.labels['certainty'][idx]\n",
    "            return text, torch.Tensor(type_tmp), torch.Tensor(polarity_tmp), torch.Tensor(tense_tmp), torch.Tensor(certainty_tmp) \n",
    "            #dictionary에 저장된 형태는 list이고 pytorch에는 tensor 형태로 넣어야하므로 torch.Tensor()을 사용\n",
    "        else:\n",
    "            return text, torch.Tensor([-1,-1,-1,-1]), torch.Tensor([-1,-1,-1]), torch.Tensor([-1,-1,-1]), torch.Tensor([-1,-1])\n",
    "            #test set의 경우 labels가 주어지지 않기 때문에 똑같은 길이지만 -1로 채운 tensor를 리턴해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module): \n",
    "    def __init__(self, base_model): \n",
    "        super().__init__() #nn.Module의 child class이므로 super().__init__()을 부르고\n",
    "        self.klue = base_model # from transformers package \n",
    "        #base_model을 통해서 받을 pretrained_model을 self.klue에 저장\n",
    "\n",
    "        #transfer learning의 기본적 방법은 중간에 hidden layer는 pretrained model의 것을 이용하고 output layer를 내가 원하는 방향으로 만들어서 training하는 것이다.\n",
    "        self.fc1 = nn.Linear(768, 32) \n",
    "        self.relu = nn.ReLU() #activation function\n",
    "        #multilabel classification이므로 각 label마다 (32,~) 으로 레이어를 만들어줌\n",
    "        #~만큼의 out_feature가 필요한 이유는 types들을 one-hot encoding해줬기 때문\n",
    "        self.type_clf = nn.Linear(32,4)\n",
    "        self.polarity_clf = nn.Linear(32,3)\n",
    "        self.tense_clf = nn.Linear(32,3)\n",
    "        self.certainty_clf = nn.Linear(32,2)\n",
    "        self.softmax = nn.Softmax(dim=1) #classification에서 많이 사용\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # input_ids : token's id / attention_mask : make a model to focus on which token\n",
    "        klue_out = self.klue(input_ids= input_ids, attention_mask = attention_mask)[0][:,0]\n",
    "                            #토큰에 해당되는 ids, 어느 토큰에 집중해야하는지 : padding에서 추가된 padding token에 대한 접근을 막기 위해\n",
    "\n",
    "        x = self.fc1(klue_out) \n",
    "        x = self.relu(x)\n",
    "\n",
    "        type_output = self.type_clf(x)\n",
    "        type_output = self.softmax(type_output)\n",
    "        polarity_output = self.polarity_clf(x)\n",
    "        polarity_output = self.softmax(polarity_output)\n",
    "        tense_output = self.tense_clf(x)\n",
    "        tense_output = self.softmax(tense_output)\n",
    "        certainty_output = self.certainty_clf(x)\n",
    "        certainty_output = self.softmax(certainty_output)\n",
    "\n",
    "        return type_output, polarity_output, tense_output, certainty_output #multilabel이므로 4개의 값을 리턴, 단순한 binary 또는 multiclassification이면 보통 1개 output만 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "def sentence_train(model, train_dataloader, val_dataloader, learning_rate, epochs, model_nm):\n",
    "    best_val_loss = 99999999999999 # setting max (act as infinity)\n",
    "    early_stopping_threshold_count = 0\n",
    "\n",
    "    criterion = {\n",
    "        'type' : nn.CrossEntropyLoss().to(device),\n",
    "        'polarity' : nn.CrossEntropyLoss().to(device),\n",
    "        'tense' : nn.CrossEntropyLoss().to(device),\n",
    "        'certainty' : nn.CrossEntropyLoss().to(device)\n",
    "    } #loss function임. 4개 다른 라벨이라서 dictionary에 4개 넣어줌. crossentropy를 통해 true값과 pred 값의 차이를 구하고 어떤 방향으로 weight를 조정해야하는지 정한다.\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0 #epoch별로 loss값이 어땠는지 기록하기 위해 사용\n",
    "        \n",
    "        model.train() # sets into the training mode \n",
    "        #val이나 test set을 모델에 넘겨줄 때 model.eval() 불러야 weights들이 업데이트 되지 않는다.\n",
    "        \n",
    "        for train_input, type_label, polarity_label, tense_label, certainty_label in tqdm(train_dataloader): #train_dataloader는 지정해준 batch_size만큼 item의 x와 y를 넘겨준다. 여기서 SentenceTypeDataset의 getitem()이 쓰임 \n",
    "            attention_mask = train_input['attention_mask'].to(device)\n",
    "            input_ids = train_input['input_ids'].squeeze(1).to(device)\n",
    "            type_label = type_label.to(device)\n",
    "            polarity_label = polarity_label.to(device)\n",
    "            tense_label = tense_label.to(device)\n",
    "            certainty_label = certainty_label.to(device)\n",
    "\n",
    "            optimizer.zero_grad() #매 epoch마다 0베이스에서 시작하게 함. 정확한 업뎃을 위해 꼭 필요\n",
    "            \n",
    "            type_output, polarity_output, tense_output, certainty_output = model(input_ids, attention_mask) # from the forward function\n",
    "            #model에 input_ids와 attention_mask를 넣어 얻은 4 값을 저장한다. 이 값들은 각 라벨마다 원핫인코딩된 컬럼에 해당될 확률들\n",
    "            \n",
    "            loss = 0.25*criterion['type'](type_output, type_label.float()) + \\\n",
    "                   0.25*criterion['polarity'](polarity_output, polarity_label.float()) + \\\n",
    "                   0.25*criterion['tense'](tense_output, tense_label.float()) + \\\n",
    "                   0.25*criterion['certainty'](certainty_output, certainty_label.float())\n",
    "            total_loss_train += loss.item() #criterion에 있는 crossentropyloss로 들어가 실제와 얼마나 유사한지 계산되고 그 계산된 값을 total_loss_train에 저장\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #backpropagation 과정을 통해 weights들을 업데이트한다.\n",
    "            \n",
    "\n",
    "        with torch.no_grad(): # since we should not change gradient for validation \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            \n",
    "            model.eval() # deactivate training\n",
    "            #with 과정을 통해 weights들을 업데이트가 아닌 선언을 한다.\n",
    "            \n",
    "            # same process as the above\n",
    "            for val_input, vtype_label, vpolarity_label, vtense_label, vcertainty_label in tqdm(val_dataloader):\n",
    "                attention_mask = val_input['attention_mask'].to(device)\n",
    "                input_ids = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                vtype_label = vtype_label.to(device)\n",
    "                vpolarity_label = vpolarity_label.to(device)\n",
    "                vtense_label = vtense_label.to(device)\n",
    "                vcertainty_label = vcertainty_label.to(device)\n",
    "                \n",
    "                vtype_output, vpolarity_output, vtense_output, vcertainty_output = model(input_ids, attention_mask) # from the forward function\n",
    "\n",
    "                loss = 0.25*criterion['type'](vtype_output, vtype_label.float()) + \\\n",
    "                        0.25*criterion['polarity'](vpolarity_output, vpolarity_label.float()) + \\\n",
    "                        0.25*criterion['tense'](vtense_output, vtense_label.float()) + \\\n",
    "                        0.25*criterion['certainty'](vcertainty_output, vcertainty_label.float())\n",
    "\n",
    "                total_loss_val += loss.item()\n",
    "                \n",
    "                #train과 같은 과정을 거치는데 여기서는 optimizer.step()이 없다. weight 업데이트를 하지 않기 때문\n",
    "\n",
    "            \n",
    "            print(f'Epochs: {epoch + 1} '\n",
    "                  f'| Train Loss: {total_loss_train / len(train_dataloader): .3f} '\n",
    "                  f'| Train Accuracy: {total_acc_train / (len(train_dataloader.dataset)): .3f} '\n",
    "                  f'| Val Loss: {total_loss_val / len(val_dataloader): .3f} '\n",
    "                  f'| Val Accuracy: {total_acc_val / len(val_dataloader.dataset): .3f}')\n",
    "            \n",
    "            if best_val_loss > total_loss_val:\n",
    "                best_val_loss = total_loss_val # saving only the best one\n",
    "                torch.save(model, f\"model/'{model_nm}'.pt\")\n",
    "                print(\"Saved model\")\n",
    "                early_stopping_threshold_count = 0\n",
    "                \n",
    "                '''\n",
    "                from pathlib import Path\n",
    "\n",
    "                current_file = Path('/home/kelly790/dacon 연습/문장분류/my1.ipynb')\n",
    "                current_file_dir = current_file.parent\n",
    "                project_root = current_file_dir.parent\n",
    "                project_root_absolute = project_root.resolve()\n",
    "                static_root_absolute = project_root_absolute / torch.save(model, f\"model/'{model_nm}'.pt\")\n",
    "                print(\"Saved model\")\n",
    "                early_stopping_threshold_count = 0\n",
    "                '''\n",
    "                \n",
    "            else:\n",
    "                early_stopping_threshold_count += 1 # checking how many epochs have passed that val_loss didn't increase\n",
    "                \n",
    "            if early_stopping_threshold_count >= 3: # ==> patience=1\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            #loss와 metric을 print하고 val_loss가 좋아졌는지 여부에 따라 모델을 저장할지 early stop할 것인지 정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>유형_대화형</th>\n",
       "      <th>유형_사실형</th>\n",
       "      <th>유형_예측형</th>\n",
       "      <th>유형_추론형</th>\n",
       "      <th>극성_긍정</th>\n",
       "      <th>극성_미정</th>\n",
       "      <th>극성_부정</th>\n",
       "      <th>시제_과거</th>\n",
       "      <th>시제_미래</th>\n",
       "      <th>시제_현재</th>\n",
       "      <th>확실성_불확실</th>\n",
       "      <th>확실성_확실</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>용산구청 관계자는 ＂재정이 열악한 지자체로서는 1800억원을 마련할 수 없다＂며 서...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>부산시는 이처럼 부산이 가파른 상승세를 보이는 이유에 대해 지난해부터 추진하고 있는...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그러나 미숙아, 만성호흡기질환, 선천 심장병, 선천 면역결핍질환, 암환자 등의 고위...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>탁구 종목에서 중국 대표팀 위상이 뛰어나기 때문이다.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이 논문에 따르면 ＇BT-11＇은 뇌의 신경전달물질인 아세틸콜린을 분해하는 효소의 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13227</th>\n",
       "      <td>우리가 익히 아는 대로 임꺽정은 신출귀몰했다.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13228</th>\n",
       "      <td>김 상무보는 ＂실제 이용자 수와 인당 사용시간 등 주요 데이터가 매년 두 자릿수 상...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13229</th>\n",
       "      <td>＇디폴트 옵션＇의 필요성을 주장해온 쪽이 항상 사례로 들어온 것이 ＇401K＇로 불...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13230</th>\n",
       "      <td>1992년부터 선양시 조선족노인협회를 후원하기 시작해 1997년에는 1500㎡ 건물...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13231</th>\n",
       "      <td>차량은 고속 상태지만 운전자는 정체모드에서 사고가 많이 발생한다.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13232 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      문장  유형_대화형  유형_사실형  \\\n",
       "0      용산구청 관계자는 ＂재정이 열악한 지자체로서는 1800억원을 마련할 수 없다＂며 서...       0       1   \n",
       "1      부산시는 이처럼 부산이 가파른 상승세를 보이는 이유에 대해 지난해부터 추진하고 있는...       0       1   \n",
       "2      그러나 미숙아, 만성호흡기질환, 선천 심장병, 선천 면역결핍질환, 암환자 등의 고위...       0       1   \n",
       "3                          탁구 종목에서 중국 대표팀 위상이 뛰어나기 때문이다.       0       0   \n",
       "4      이 논문에 따르면 ＇BT-11＇은 뇌의 신경전달물질인 아세틸콜린을 분해하는 효소의 ...       0       1   \n",
       "...                                                  ...     ...     ...   \n",
       "13227                          우리가 익히 아는 대로 임꺽정은 신출귀몰했다.       0       1   \n",
       "13228  김 상무보는 ＂실제 이용자 수와 인당 사용시간 등 주요 데이터가 매년 두 자릿수 상...       0       1   \n",
       "13229  ＇디폴트 옵션＇의 필요성을 주장해온 쪽이 항상 사례로 들어온 것이 ＇401K＇로 불...       1       0   \n",
       "13230  1992년부터 선양시 조선족노인협회를 후원하기 시작해 1997년에는 1500㎡ 건물...       0       1   \n",
       "13231               차량은 고속 상태지만 운전자는 정체모드에서 사고가 많이 발생한다.       0       1   \n",
       "\n",
       "       유형_예측형  유형_추론형  극성_긍정  극성_미정  극성_부정  시제_과거  시제_미래  시제_현재  확실성_불확실  \\\n",
       "0           0       0      1      0      0      1      0      0        0   \n",
       "1           0       0      1      0      0      1      0      0        0   \n",
       "2           0       0      1      0      0      0      0      1        0   \n",
       "3           0       1      1      0      0      0      0      1        0   \n",
       "4           0       0      1      0      0      0      0      1        0   \n",
       "...       ...     ...    ...    ...    ...    ...    ...    ...      ...   \n",
       "13227       0       0      1      0      0      1      0      0        0   \n",
       "13228       0       0      1      0      0      1      0      0        0   \n",
       "13229       0       0      1      0      0      0      0      1        0   \n",
       "13230       0       0      1      0      0      1      0      0        0   \n",
       "13231       0       0      1      0      0      0      0      1        0   \n",
       "\n",
       "       확실성_확실  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "...       ...  \n",
       "13227       1  \n",
       "13228       1  \n",
       "13229       1  \n",
       "13230       1  \n",
       "13231       1  \n",
       "\n",
       "[13232 rows x 13 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tmp = train[['문장', '유형', '극성', '시제', '확실성']] #label을 제외한 나머지 컬럼들만 킵하고 \n",
    "train_tmp = pd.get_dummies(train_tmp, columns=['유형', '극성', '시제', '확실성']) #pd.get_dummies로 원핫인코딩\n",
    "train_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 라벨별 뽑아서 train_labels에 dictionary 형태로 저장\n",
    "train_type = train_tmp.iloc[:,1:5].values.tolist()\n",
    "train_polarity = train_tmp.iloc[:,5:8].values.tolist()\n",
    "train_tense = train_tmp.iloc[:,8:11].values.tolist()\n",
    "train_certainty = train_tmp.iloc[:,11:13].values.tolist()\n",
    "train_labels = {\n",
    "    'type': train_type,\n",
    "    'polarity': train_polarity,\n",
    "    'tense': train_tense,\n",
    "    'certainty': train_certainty\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tmp = val[['문장', '유형', '극성', '시제', '확실성']]\n",
    "val_tmp = pd.get_dummies(val_tmp, columns=['유형', '극성', '시제', '확실성'])\n",
    "\n",
    "val_type = val_tmp.iloc[:,1:5].values.tolist()\n",
    "val_polarity = val_tmp.iloc[:,5:8].values.tolist()\n",
    "val_tense = val_tmp.iloc[:,8:11].values.tolist()\n",
    "val_certainty = val_tmp.iloc[:,11:13].values.tolist()\n",
    "val_labels = {\n",
    "    'type': val_type,\n",
    "    'polarity': val_polarity,\n",
    "    'tense': val_tense,\n",
    "    'certainty': val_certainty\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(SentenceTypeDataset(train_tmp, tokenizer, train_labels), batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0) # num_workers: how many subprocesses to use for data loading  \n",
    "val_dataloader = DataLoader(SentenceTypeDataset(val_tmp, tokenizer, val_labels), batch_size=CFG['BATCH_SIZE'], num_workers=0)\n",
    "\n",
    "#데이터셋이 준비가 되었으므로 SentenceTypeDataset에 dataframe, tokenizer, labels들을 넣고 batch_size 정하고 dataloader에 넣는다.\n",
    "#dataloader는 지정된 batch만큼 items를 모델에 넘겨주어 training 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceClassifier(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 827/827 [00:30<00:00, 27.21it/s]\n",
      "100%|██████████| 207/207 [00:02<00:00, 99.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.758 | Train Accuracy:  0.000 | Val Loss:  0.683 | Val Accuracy:  0.000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory model does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sentence_train(model, train_dataloader, val_dataloader, CFG[\u001b[39m'\u001b[39;49m\u001b[39mLEARNING_RATE\u001b[39;49m\u001b[39m'\u001b[39;49m], CFG[\u001b[39m'\u001b[39;49m\u001b[39mEPOCHS\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m'\u001b[39;49m\u001b[39mkclue\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[22], line 94\u001b[0m, in \u001b[0;36msentence_train\u001b[0;34m(model, train_dataloader, val_dataloader, learning_rate, epochs, model_nm)\u001b[0m\n\u001b[1;32m     92\u001b[0m project_root \u001b[39m=\u001b[39m current_file_dir\u001b[39m.\u001b[39mparent\n\u001b[1;32m     93\u001b[0m project_root_absolute \u001b[39m=\u001b[39m project_root\u001b[39m.\u001b[39mresolve()\n\u001b[0;32m---> 94\u001b[0m static_root_absolute \u001b[39m=\u001b[39m project_root_absolute \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39;49msave(model, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodel/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_nm\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     95\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSaved model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m early_stopping_threshold_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/torch/serialization.py:422\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    419\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 422\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    424\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/torch/serialization.py:309\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/torch/serialization.py:287\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m     \u001b[39msuper\u001b[39m(_open_zipfile_writer_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory model does not exist."
     ]
    }
   ],
   "source": [
    "sentence_train(model, train_dataloader, val_dataloader, CFG['LEARNING_RATE'], CFG['EPOCHS'], 'kclue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "492527acb83a23147d10ea222e8b25a5f747a0f75be240338053b95d77080704"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
