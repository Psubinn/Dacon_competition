{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS': 10,\n",
    "    'LEARNING_RATE':1e-4,\n",
    "    'BATCH_SIZE': 256,\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fixed randomseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/kelly790/dacon_ex/train.csv')\n",
    "test = pd.read_csv('/home/kelly790/dacon_ex/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>문장</th>\n",
       "      <th>유형</th>\n",
       "      <th>극성</th>\n",
       "      <th>시제</th>\n",
       "      <th>확실성</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>0.75%포인트 금리 인상은 1994년 이후 28년 만에 처음이다.</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>이어 ＂앞으로 전문가들과 함께 4주 단위로 상황을 재평가할 예정＂이라며 ＂그 이전이...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>정부가 고유가 대응을 위해 7월부터 연말까지 유류세 인하 폭을 30%에서 37%까지...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>미래</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-미래-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>서울시는 올해 3월 즉시 견인 유예시간 60분을 제공하겠다고 밝혔지만, 하루 만에 ...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>익사한 자는 사다리에 태워 거꾸로 놓고 소금으로 코를 막아 가득 채운다.</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16536</th>\n",
       "      <td>TRAIN_16536</td>\n",
       "      <td>＇신동덤＇은 ＇신비한 동물사전＇과 ＇해리 포터＇ 시리즈를 잇는 마법 어드벤처물로, ...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16537</th>\n",
       "      <td>TRAIN_16537</td>\n",
       "      <td>수족냉증은 어릴 때부터 심했으며 관절은 어디 한 곳이 아니고 목, 어깨, 팔꿈치, ...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16538</th>\n",
       "      <td>TRAIN_16538</td>\n",
       "      <td>김금희 소설가는 ＂계약서 조정이 그리 어려운가 작가를 격려한다면서 그런 문구 하나 ...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16539</th>\n",
       "      <td>TRAIN_16539</td>\n",
       "      <td>1만명이 넘는 방문자수를 기록한 이번 전시회는 총 77개 작품을 넥슨 사옥을 그대로...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>불확실</td>\n",
       "      <td>사실형-긍정-과거-불확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16540</th>\n",
       "      <td>TRAIN_16540</td>\n",
       "      <td>《목민심서》의 내용이다.</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16541 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                                 문장   유형  \\\n",
       "0      TRAIN_00000              0.75%포인트 금리 인상은 1994년 이후 28년 만에 처음이다.  사실형   \n",
       "1      TRAIN_00001  이어 ＂앞으로 전문가들과 함께 4주 단위로 상황을 재평가할 예정＂이라며 ＂그 이전이...  사실형   \n",
       "2      TRAIN_00002  정부가 고유가 대응을 위해 7월부터 연말까지 유류세 인하 폭을 30%에서 37%까지...  사실형   \n",
       "3      TRAIN_00003  서울시는 올해 3월 즉시 견인 유예시간 60분을 제공하겠다고 밝혔지만, 하루 만에 ...  사실형   \n",
       "4      TRAIN_00004           익사한 자는 사다리에 태워 거꾸로 놓고 소금으로 코를 막아 가득 채운다.  사실형   \n",
       "...            ...                                                ...  ...   \n",
       "16536  TRAIN_16536  ＇신동덤＇은 ＇신비한 동물사전＇과 ＇해리 포터＇ 시리즈를 잇는 마법 어드벤처물로, ...  사실형   \n",
       "16537  TRAIN_16537  수족냉증은 어릴 때부터 심했으며 관절은 어디 한 곳이 아니고 목, 어깨, 팔꿈치, ...  사실형   \n",
       "16538  TRAIN_16538  김금희 소설가는 ＂계약서 조정이 그리 어려운가 작가를 격려한다면서 그런 문구 하나 ...  사실형   \n",
       "16539  TRAIN_16539  1만명이 넘는 방문자수를 기록한 이번 전시회는 총 77개 작품을 넥슨 사옥을 그대로...  사실형   \n",
       "16540  TRAIN_16540                                      《목민심서》의 내용이다.  사실형   \n",
       "\n",
       "       극성  시제  확실성          label  \n",
       "0      긍정  현재   확실   사실형-긍정-현재-확실  \n",
       "1      긍정  과거   확실   사실형-긍정-과거-확실  \n",
       "2      긍정  미래   확실   사실형-긍정-미래-확실  \n",
       "3      긍정  과거   확실   사실형-긍정-과거-확실  \n",
       "4      긍정  현재   확실   사실형-긍정-현재-확실  \n",
       "...    ..  ..  ...            ...  \n",
       "16536  긍정  과거   확실   사실형-긍정-과거-확실  \n",
       "16537  긍정  과거   확실   사실형-긍정-과거-확실  \n",
       "16538  긍정  과거   확실   사실형-긍정-과거-확실  \n",
       "16539  긍정  과거  불확실  사실형-긍정-과거-불확실  \n",
       "16540  긍정  현재   확실   사실형-긍정-현재-확실  \n",
       "\n",
       "[16541 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 학습데이터를 학습 / 검증 데이터셋으로 재 분할\n",
    "train, val, _, _ = train_test_split(df, df['label'], test_size=0.2, random_state=CFG['SEED'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 문장(text) 벡터화\n",
    "\n",
    "2. label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13232, 9439) (3309, 9439) (7090, 9439)\n"
     ]
    }
   ],
   "source": [
    "# 1. 문장(Text) 벡터화 -> TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df = 4, analyzer = 'word', ngram_range=(1, 2))\n",
    "vectorizer.fit(np.array(train[\"문장\"]))\n",
    "\n",
    "train_vec = vectorizer.transform(train[\"문장\"])\n",
    "val_vec = vectorizer.transform(val[\"문장\"])\n",
    "test_vec = vectorizer.transform(test[\"문장\"])\n",
    "\n",
    "print(train_vec.shape, val_vec.shape, test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Label Encoding (유형, 극성, 시제, 확실성)\n",
    "type_le = LabelEncoder()\n",
    "train[\"유형\"] = type_le.fit_transform(train[\"유형\"].values)\n",
    "val[\"유형\"] = type_le.transform(val[\"유형\"].values)\n",
    "\n",
    "polarity_le = LabelEncoder()\n",
    "train[\"극성\"] = polarity_le.fit_transform(train[\"극성\"].values)\n",
    "val[\"극성\"] = polarity_le.transform(val[\"극성\"].values)\n",
    "\n",
    "tense_le = LabelEncoder()\n",
    "train[\"시제\"] = tense_le.fit_transform(train[\"시제\"].values)\n",
    "val[\"시제\"] = tense_le.transform(val[\"시제\"].values)\n",
    "\n",
    "certainty_le = LabelEncoder()\n",
    "train[\"확실성\"] = certainty_le.fit_transform(train[\"확실성\"].values)\n",
    "val[\"확실성\"] = certainty_le.transform(val[\"확실성\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type = train[\"유형\"].values # sentence type\n",
    "train_polarity = train[\"극성\"].values # sentence polarity\n",
    "train_tense = train[\"시제\"].values # sentence tense\n",
    "train_certainty = train[\"확실성\"].values # sentence certainty\n",
    "\n",
    "train_labels = {\n",
    "    'type' : train_type,\n",
    "    'polarity' : train_polarity,\n",
    "    'tense' : train_tense,\n",
    "    'certainty' : train_certainty\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_type = val[\"유형\"].values # sentence type\n",
    "val_polarity = val[\"극성\"].values # sentence polarity\n",
    "val_tense = val[\"시제\"].values # sentence tense\n",
    "val_certainty = val[\"확실성\"].values # sentence certainty\n",
    "\n",
    "val_labels = {\n",
    "    'type' : val_type,\n",
    "    'polarity' : val_polarity,\n",
    "    'tense' : val_tense,\n",
    "    'certainty' : val_certainty\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, st_vec, st_labels):\n",
    "        self.st_vec = st_vec\n",
    "        self.st_labels = st_labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        st_vector = torch.FloatTensor(self.st_vec[index].toarray()).squeeze(0)\n",
    "        if self.st_labels is not None:\n",
    "            st_type = self.st_labels['type'][index]\n",
    "            st_polarity = self.st_labels['polarity'][index]\n",
    "            st_tense = self.st_labels['tense'][index]\n",
    "            st_certainty = self.st_labels['certainty'][index]\n",
    "            return st_vector, st_type, st_polarity, st_tense, st_certainty\n",
    "        else:\n",
    "            return st_vector\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.st_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_vec, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_vec, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, input_dim=9439):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.feature_extract = nn.Sequential(\n",
    "            nn.Linear(in_features=input_dim, out_features=1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.type_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=4),\n",
    "        )\n",
    "        self.polarity_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=3),\n",
    "        )\n",
    "        self.tense_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=3),\n",
    "        )\n",
    "        self.certainty_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=2),\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # 문장 유형, 극성, 시제, 확실성을 각각 분류\n",
    "        type_output = self.type_classifier(x)\n",
    "        polarity_output = self.polarity_classifier(x)\n",
    "        tense_output = self.tense_classifier(x)\n",
    "        certainty_output = self.certainty_classifier(x)\n",
    "        return type_output, polarity_output, tense_output, certainty_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = {\n",
    "        'type' : nn.CrossEntropyLoss().to(device),\n",
    "        'polarity' : nn.CrossEntropyLoss().to(device),\n",
    "        'tense' : nn.CrossEntropyLoss().to(device),\n",
    "        'certainty' : nn.CrossEntropyLoss().to(device)\n",
    "    }\n",
    "    \n",
    "    best_loss = 999999\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for sentence, type_label, polarity_label, tense_label, certainty_label in tqdm(iter(train_loader)):\n",
    "            sentence = sentence.to(device)\n",
    "            type_label = type_label.to(device)\n",
    "            polarity_label = polarity_label.to(device)\n",
    "            tense_label = tense_label.to(device)\n",
    "            certainty_label = certainty_label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            type_logit, polarity_logit, tense_logit, certainty_logit = model(sentence)\n",
    "            \n",
    "            loss = 0.25 * criterion['type'](type_logit, type_label) + \\\n",
    "                    0.25 * criterion['polarity'](polarity_logit, polarity_label) + \\\n",
    "                    0.25 * criterion['tense'](tense_logit, tense_label) + \\\n",
    "                    0.25 * criterion['certainty'](certainty_logit, certainty_label)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        val_loss, val_type_f1, val_polarity_f1, val_tense_f1, val_certainty_f1 = validation(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}] 유형 F1 : [{val_type_f1:.5f}] 극성 F1 : [{val_polarity_f1:.5f}] 시제 F1 : [{val_tense_f1:.5f}] 확실성 F1 : [{val_certainty_f1:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    \n",
    "    type_preds, polarity_preds, tense_preds, certainty_preds = [], [], [], []\n",
    "    type_labels, polarity_labels, tense_labels, certainty_labels = [], [], [], []\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentence, type_label, polarity_label, tense_label, certainty_label in tqdm(iter(val_loader)):\n",
    "            sentence = sentence.to(device)\n",
    "            type_label = type_label.to(device)\n",
    "            polarity_label = polarity_label.to(device)\n",
    "            tense_label = tense_label.to(device)\n",
    "            certainty_label = certainty_label.to(device)\n",
    "            \n",
    "            type_logit, polarity_logit, tense_logit, certainty_logit = model(sentence)\n",
    "            \n",
    "            loss = 0.25 * criterion['type'](type_logit, type_label) + \\\n",
    "                    0.25 * criterion['polarity'](polarity_logit, polarity_label) + \\\n",
    "                    0.25 * criterion['tense'](tense_logit, tense_label) + \\\n",
    "                    0.25 * criterion['certainty'](certainty_logit, certainty_label)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            type_preds += type_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            type_labels += type_label.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            polarity_preds += polarity_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            polarity_labels += polarity_label.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            tense_preds += tense_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            tense_labels += tense_label.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            certainty_preds += certainty_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            certainty_labels += certainty_label.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    type_f1 = f1_score(type_labels, type_preds, average='weighted')\n",
    "    polarity_f1 = f1_score(polarity_labels, polarity_preds, average='weighted')\n",
    "    tense_f1 = f1_score(tense_labels, tense_preds, average='weighted')\n",
    "    certainty_f1 = f1_score(certainty_labels, certainty_preds, average='weighted')\n",
    "    \n",
    "    return np.mean(val_loss), type_f1, polarity_f1, tense_f1, certainty_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m CFG[\u001b[39m\"\u001b[39m\u001b[39mLEARNING_RATE\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,threshold_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mabs\u001b[39m\u001b[39m'\u001b[39m,min_lr\u001b[39m=\u001b[39m\u001b[39m1e-8\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m infer_model \u001b[39m=\u001b[39m train(model, optimizer, train_loader, val_loader, scheduler, device)\n",
      "Cell \u001b[0;32mIn[61], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, val_loader, scheduler, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m type_logit, polarity_logit, tense_logit, certainty_logit \u001b[39m=\u001b[39m model(sentence)\n\u001b[1;32m     28\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0.25\u001b[39m \u001b[39m*\u001b[39m criterion[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m](type_logit, type_label) \u001b[39m+\u001b[39m \\\n\u001b[1;32m     29\u001b[0m         \u001b[39m0.25\u001b[39m \u001b[39m*\u001b[39m criterion[\u001b[39m'\u001b[39m\u001b[39mpolarity\u001b[39m\u001b[39m'\u001b[39m](polarity_logit, polarity_label) \u001b[39m+\u001b[39m \\\n\u001b[1;32m     30\u001b[0m         \u001b[39m0.25\u001b[39m \u001b[39m*\u001b[39m criterion[\u001b[39m'\u001b[39m\u001b[39mtense\u001b[39m\u001b[39m'\u001b[39m](tense_logit, tense_label) \u001b[39m+\u001b[39m \\\n\u001b[1;32m     31\u001b[0m         \u001b[39m0.25\u001b[39m \u001b[39m*\u001b[39m criterion[\u001b[39m'\u001b[39m\u001b[39mcertainty\u001b[39m\u001b[39m'\u001b[39m](certainty_logit, certainty_label)\n\u001b[0;32m---> 33\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     34\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m train_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = BaseModel()\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "492527acb83a23147d10ea222e8b25a5f747a0f75be240338053b95d77080704"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
